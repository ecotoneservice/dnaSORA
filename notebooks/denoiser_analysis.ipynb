{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "import io\n",
    "from PIL import Image, ImageChops\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "\n",
    "import panel as pn\n",
    "from PIL import Image, ImageOps\n",
    "import holoviews as hv\n",
    "import altair as alt\n",
    "from sklearn.decomposition import PCA\n",
    "alt.data_transformers.disable_max_rows()\n",
    "hv.extension(\"plotly\")\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "pn.extension(\"plotly\")\n",
    "pn.config.theme = 'dark'\n",
    "import plotly.io as pio\n",
    "hv.renderer('plotly').theme = 'dark'\n",
    "import plotly.express as px\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from datetime import datetime\n",
    "from functools import partial, reduce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "import torch\n",
    "import functools\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from tqdm import tqdm, trange\n",
    "import lpips\n",
    "import lpips\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.v2 import PILToTensor,Compose\n",
    "import torchvision\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from src.models.denoiser import DenoiserModelPipeline\n",
    "from src.md import generate_gausian_data\n",
    "from src.preprocessing import ChromoDataContainer, ChromoData\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook helps to perform the analysis of the model\n",
    "# Various plots and visualizations are generated to understand the model performance and use plots in the paper\n",
    "\n",
    "# Model name which will perform the analysing\n",
    "trained_model_name = \"full_labels_psize_4\"\n",
    "\n",
    "pipeline = DenoiserModelPipeline.load(trained_model_name)\n",
    "pipeline.model.to(device)\n",
    "pipeline.model.eval()\n",
    "\n",
    "chromo_data = ChromoDataContainer.load_from_pkl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISZATION FOR NOIZE STEPS\n",
    "# paper plot\n",
    "# scattrplots of SD and noize on different steps\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Modeling\n",
    "\n",
    "T = 1000\n",
    "# Forward diffusion calculation parameters\n",
    "betas = torch.linspace(0.0001, 0.02, T)  # (T,)\n",
    "alphas = 1 - betas  # (T,)\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=-1)  # Cumulative product of alpha_t (T,) [a1, a2, a3, ....] -> [a1, a1*a2, a1*a2*a3, .....]\n",
    "alphas_cumprod_prev = torch.cat((torch.tensor([1.0]), alphas_cumprod[:-1]), dim=-1)  # Cumulative product of alpha_t-1 (T,), [1, a1, a1*a2, a1*a2*a3, .....]\n",
    "variance = (1 - alphas) * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod)  # Variance used for denoising (T,)\n",
    "\n",
    "max_label = pipeline.config.model.parameters.label_num // 2\n",
    "img_size = pipeline.config.model.parameters.img_size\n",
    "perform_ablation = True\n",
    "single_sample = True\n",
    "\n",
    "def backward_denoise(model, x, y, variance, clamp=False, perform_ablation=False, start_step=0):\n",
    "    \n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.tensor(x).to(device)\n",
    "        \n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = torch.tensor(y).to(device)\n",
    "        \n",
    "    if isinstance(variance, np.ndarray):\n",
    "        variance = torch.tensor(variance).to(device)\n",
    "        \n",
    "    steps_noized = []\n",
    "    steps=[]\n",
    "    noize_preds=[]\n",
    "    denoized_unclamp=[]\n",
    "    denoized_clamp=[]\n",
    "    ablation_data = []\n",
    "    ablation_timesteps = range(0,1000)\n",
    "\n",
    "    global alphas,alphas_cumprod\n",
    "\n",
    "    x=x.to(device)\n",
    "    alphas=alphas.to(device)\n",
    "    alphas_cumprod=alphas_cumprod.to(device)\n",
    "    variance=variance.to(device)\n",
    "    y=y.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for time in range(start_step, -1, -1):\n",
    "            steps_noized.append(x.clone())\n",
    "            \n",
    "            t=torch.full((x.size(0),),time).to(device) \n",
    "            \n",
    "            do_ablation_this_iter = time in ablation_timesteps and perform_ablation\n",
    "    \n",
    "            noise, ablation=model(x,t,y, do_ablation_this_iter)\n",
    "            noize_preds.append(noise.clone())\n",
    "            if do_ablation_this_iter:\n",
    "                ablation_data.append(ablation)\n",
    "            \n",
    "            shape=(x.size(0),1,1,1) \n",
    "            mean=1/torch.sqrt(alphas[t].view(*shape))*  \\\n",
    "                (\n",
    "                    x-\n",
    "                    (1-alphas[t].view(*shape))/torch.sqrt(1-alphas_cumprod[t].view(*shape))*noise\n",
    "                )\n",
    "            if time!=0:\n",
    "                x=mean+ \\\n",
    "                    torch.randn_like(x)* \\\n",
    "                    torch.sqrt(variance[t].view(*shape))\n",
    "            else:\n",
    "                x=mean\n",
    "            denoized_unclamp.append(x.clone())\n",
    "                \n",
    "            if clamp:\n",
    "                x=torch.clamp(x, -1.0, 1.0).detach()\n",
    "            else:\n",
    "                x = x.detach()\n",
    "            \n",
    "            denoized_clamp.append(x.clone())\n",
    "        \n",
    "            steps.append(x)\n",
    "    return steps, steps_noized, noize_preds, denoized_unclamp, denoized_clamp, ablation_data\n",
    "\n",
    "def generate_sample(score_model, y, init_frame=None, strength=1., clamp=False, img_size=28, perform_ablation=False, start_step_arg=T-1):\n",
    "    variance_arg=variance.clone()\n",
    "    if init_frame is None:\n",
    "        x=torch.randn(size=(1,1,img_size,img_size))\n",
    "        # x=torch.clamp(x,-1,1)\n",
    "        start_step = start_step_arg-1\n",
    "    else:\n",
    "        x = init_frame * 2 - 1  # Scale to [-1, 1]\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # Calculate starting timestep based on strength\n",
    "        start_step = int(start_step_arg * strength)-1\n",
    "        \n",
    "        # Add the appropriate amount of noise for the starting timestep\n",
    "        noise = torch.randn_like(x)\n",
    "        alpha_cumprod = alphas_cumprod[start_step-1]\n",
    "        x = torch.sqrt(alpha_cumprod) * x + torch.sqrt(1 - alpha_cumprod) * noise\n",
    "        \n",
    "        # rescale to [-1, 1]\n",
    "        x = torch.clamp(x, -1, 1)\n",
    "        \n",
    "    y = torch.tensor([y])\n",
    "    steps, steps_noized, noize_preds, denoized_unclamp, denoized_clamp, ablation_data = backward_denoise(\n",
    "        score_model,\n",
    "        x,\n",
    "        y, \n",
    "        variance_arg, \n",
    "        clamp=clamp, \n",
    "        perform_ablation=perform_ablation, \n",
    "        start_step=start_step\n",
    "    )\n",
    "    \n",
    "    # this returns from -1;1 to 0;1\n",
    "    final_img=(steps[-1][0].to('cpu')+1)/2\n",
    "    \n",
    "    \n",
    "    return final_img, steps, steps_noized, noize_preds, denoized_unclamp, denoized_clamp, ablation_data\n",
    "\n",
    "\n",
    "def recover_df_from_tensor(tensor):\n",
    "    if tensor is None:\n",
    "        return []\n",
    "    # remove dim 1\n",
    "    tensor = np.squeeze(tensor, axis=1)\n",
    "    # convert to numpy\n",
    "    tensor = tensor.cpu().detach().numpy()\n",
    "    # convert to df\n",
    "    dfs = []\n",
    "    for i in range(tensor.shape[0]):\n",
    "        gxx_vals = tensor[i]\n",
    "        # flatten\n",
    "        gxx_vals = gxx_vals.flatten()\n",
    "        # create df\n",
    "        df = pd.DataFrame({\n",
    "            'POS_bin': np.arange(gxx_vals.shape[0]),\n",
    "            'Gxx_ratio': gxx_vals,\n",
    "        })\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "def generate_full_sequence(score_model, y_max=24, init_frame=None, strength=0.5, clamp=False, img_size=28, perform_ablation=False, single_label=False, hide_tqdm=False, start_step_arg=T):\n",
    "    \n",
    "    def recover_dfs_and_concat(tensors):\n",
    "        dfs = []\n",
    "        for tensor in tensors:\n",
    "            dfs.extend(recover_df_from_tensor(tensor))\n",
    "        df = pd.concat(dfs)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    all_gxxs = []\n",
    "    all_steps = []\n",
    "    all_steps_noized = []\n",
    "    all_noize_preds = []\n",
    "    all_denoized_unclamp = []\n",
    "    all_denoized_clamp = []\n",
    "    all_ablation_data = []\n",
    "    if single_label:\n",
    "        total_lbs = [y_max]\n",
    "    else:\n",
    "        total_lbs = range(y_max)\n",
    "    for i in tqdm(total_lbs, disable=hide_tqdm):\n",
    "        samples, steps, steps_noized, noize_preds, denoized_unclamp, denoized_clamp, ablation_data = generate_sample(score_model, y=i, init_frame=init_frame, strength=strength, clamp=clamp, img_size=img_size, perform_ablation=perform_ablation, start_step_arg=start_step_arg)\n",
    "        all_gxxs.append(samples)\n",
    "        all_steps.append(steps)\n",
    "        all_steps_noized.append(steps_noized)\n",
    "        all_noize_preds.append(noize_preds)\n",
    "        all_denoized_unclamp.append(denoized_unclamp)\n",
    "        all_denoized_clamp.append(denoized_clamp)\n",
    "        all_ablation_data.append(ablation_data)\n",
    "        \n",
    "    \n",
    "    df_gxxs = recover_dfs_and_concat(all_gxxs)\n",
    "    return df_gxxs, all_steps, all_steps_noized, all_noize_preds, all_denoized_unclamp, all_denoized_clamp, all_ablation_data\n",
    "      \n",
    "\n",
    "chromo_dfs = []\n",
    "for i in tqdm(range(0, 768, 28)):\n",
    "    df_gxxs, all_steps, all_noized_steps, all_noize_preds, all_denoized_unclamp, all_denoized_clamp, all_ablation_data = generate_full_sequence(pipeline.model, y_max=i, clamp=False, img_size=img_size, single_label=True, hide_tqdm=True)\n",
    "    chromo_dfs.append(df_gxxs)\n",
    "\n",
    "sns.set_theme(style='white', font_scale=1.5)\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "# max 23\n",
    "indexes_to_sample = [0]\n",
    "# steps_to_show = [700, 750, 800, 850, 900, 950, 999]\n",
    "# steps_to_show = [400, 600, 800, 900, 925, 950, 975, 985, 999]\n",
    "steps_to_show = [400, 800, 925, 975, 999]\n",
    "\n",
    "# sample chunk onformation\n",
    "pred_noized_vals = [all_noized_steps[i] for i in indexes_to_sample]\n",
    "pred_noize_vals = [all_noize_preds[i] for i in indexes_to_sample]\n",
    "pred_unc_vals = [all_denoized_unclamp[i] for i in indexes_to_sample]\n",
    "pred_clamp_vals = [all_denoized_clamp[i] for i in indexes_to_sample]\n",
    "\n",
    "# sample steps from each chunk\n",
    "pred_noized_vals = [[pred_noized_vals[i][j] for j in steps_to_show] for i in range(len(pred_noized_vals))]\n",
    "pred_noize_vals = [[pred_noize_vals[i][j] for j in steps_to_show] for i in range(len(pred_noize_vals))]\n",
    "pred_unc_vals = [[pred_unc_vals[i][j] for j in steps_to_show] for i in range(len(pred_unc_vals))]\n",
    "pred_clamp_vals = [[pred_clamp_vals[i][j] for j in steps_to_show] for i in range(len(pred_clamp_vals))]\n",
    "\n",
    "# convert tesnors on each step to df\n",
    "pred_noized_vals = [[recover_df_from_tensor(pred_noized_vals[i][j]) for j in range(len(pred_noized_vals[i]))] for i in range(len(pred_noized_vals))]\n",
    "pred_noize_vals = [[recover_df_from_tensor(pred_noize_vals[i][j]) for j in range(len(pred_noize_vals[i]))] for i in range(len(pred_noize_vals))]\n",
    "pred_unc_vals = [[recover_df_from_tensor(pred_unc_vals[i][j]) for j in range(len(pred_unc_vals[i]))] for i in range(len(pred_unc_vals))]\n",
    "pred_clamp_vals = [[recover_df_from_tensor(pred_clamp_vals[i][j]) for j in range(len(pred_clamp_vals[i]))] for i in range(len(pred_clamp_vals))]\n",
    "\n",
    "# zip and join in [[pred_noize_vals, pred_unc_vals, pred_clamp_vals]]\n",
    "pred_vals = list(zip(\n",
    "    pred_noized_vals, \n",
    "    pred_noize_vals, \n",
    "    pred_unc_vals,\n",
    "    pred_clamp_vals\n",
    "))\n",
    "len_charts = len(pred_vals[0])\n",
    "len_charts = 2\n",
    "\n",
    "# plot images\n",
    "fig, axes = plt.subplots(len(pred_vals) * len_charts, len(steps_to_show), figsize=(15, 6))\n",
    "type_name = ['Noised', 'Noise', 'Unclamp', 'Clamp']\n",
    "type_colors = ['red', 'blue', 'green', 'purple']\n",
    "# Add letters to mark rows\n",
    "row_letters = ['A)', 'B)', 'C)', 'D)']\n",
    "\n",
    "for i in range(len(pred_vals)):\n",
    "    for j in range(len_charts):\n",
    "        for k in range(len(steps_to_show)):\n",
    "            dfs = pred_vals[i][j][k]\n",
    "            if len (dfs) == 0:\n",
    "                continue\n",
    "            df = dfs[0]\n",
    "            ax=axes[i*len_charts + j, k]\n",
    "            \n",
    "            # Add row letters to first column\n",
    "            if k == 0:\n",
    "                ax.text(-0.2, 0.91, row_letters[i*len_charts + j], \n",
    "                        transform=ax.transAxes,\n",
    "                        fontsize=24,\n",
    "                        fontweight='bold',\n",
    "                        fontfamily='Times New Roman')\n",
    "            \n",
    "            \n",
    "            # if k == 0:\n",
    "            # ax.set_title(f\"Chunk {indexes_to_sample[i]} Image {k} {type_name[j]}\")\n",
    "            ax.set_title(f\"\")\n",
    "            sns.scatterplot(data=df, x='POS_bin', y='Gxx_ratio', color=type_colors[j], ax=ax, s=7)\n",
    "            # hide y axis on all but first and last\n",
    "            # if k>0 or k < len(steps_to_show):\n",
    "                # ax.yaxis.set_visible(False)\n",
    "            # else:\n",
    "            ax.yaxis.set_visible(False)\n",
    "            ax.xaxis.set_visible(False)\n",
    "            # ax.set_yticks([-2, -1, 0, 1, 2])\n",
    "            ax.set_yticks([],[])\n",
    "            ax.set_xticks([],[])\n",
    "            if j == len_charts - 1:\n",
    "                ax.xaxis.set_visible(True)\n",
    "                ax.set_xlabel(f\"Step {steps_to_show[k]}\")\n",
    "                ax.set_ylabel(f\"\")\n",
    "            else:\n",
    "                ax.set_xticks([],[])\n",
    "                ax.set_xlabel(\"\")\n",
    "                ax.set_ylabel(f\"\") \n",
    "\n",
    "# # put axes on the top\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISATION OF DENOISING PROCESS\n",
    "# plots of SD and noize on different steps, with grayscale image visualisation for SD data\n",
    "# scaled 50% of original paper size to fit here inline\n",
    "\n",
    "pn.config.theme = 'default'\n",
    "pn.extension()\n",
    "sns.color_palette(\"tab10\")\n",
    "hv.extension(\"bokeh\")\n",
    "def get_paper_denoising_vis(label, index):\n",
    "\n",
    "    df_gxxs, all_steps, all_noized_steps, all_noize_preds, all_denoized_unclamp, all_denoized_clamp, all_ablation_data = generate_full_sequence(\n",
    "        pipeline.model, \n",
    "        y_max=label, \n",
    "        clamp=False, \n",
    "        img_size=img_size, \n",
    "        single_label=True, \n",
    "        hide_tqdm=True)\n",
    "\n",
    "    # plot holoviews scatterplot\n",
    "    scatter = hv.Scatter(df_gxxs, kdims=['POS_bin'], vdims=['Gxx_ratio'])\n",
    "    # hide controls, make non interactive\n",
    "    scatter.opts(\n",
    "        width=150, \n",
    "        height=150,\n",
    "        size=2,\n",
    "        show_grid=False,\n",
    "        show_legend=False,\n",
    "        toolbar=None,\n",
    "        xlabel='',  # hide x label\n",
    "        ylabel='',  # hide y label\n",
    "        xaxis=None, # hide x axis\n",
    "        yaxis=None, # hide y axis\n",
    "        bgcolor='white',\n",
    "        show_frame=False,  # Hide frame\n",
    "        tools=[],\n",
    "        active_tools=[]\n",
    "    )\n",
    "\n",
    "    # take first 3, middle 3, last 3\n",
    "    el_to_take = 3\n",
    "    indexes_to_take = list(range(0, el_to_take)) + list(range(len(df_gxxs) // 2 - el_to_take, len(df_gxxs) // 2)) + list(range(len(df_gxxs) - el_to_take, len(df_gxxs)))\n",
    "    values_of_indexes = df_gxxs.iloc[indexes_to_take]\n",
    "\n",
    "    # create squares holovis with gray value of values\n",
    "    squares = [pn.pane.HTML(width=15, height=15, styles={\n",
    "        'background': f'rgb({int(255*val)},{int(255*val)},{int(255*val)})',\n",
    "        'border': '1px solid black',\n",
    "    }) for val in values_of_indexes['Gxx_ratio']]\n",
    "    # split squares by groups of el_to_take\n",
    "    squares_flatten = [pn.Row(*squares[i:i+el_to_take]) for i in range(0, len(squares), el_to_take)]\n",
    "    \n",
    "    # create arrow for denoising reshaper\n",
    "    arrow_down = pn.widgets.ButtonIcon(icon='arrow-down', size='4em')\n",
    "    \n",
    "    # caption\n",
    "    text_simple_visualization = pn.pane.Markdown(\"How process reshapes flat Chromo to 2d array\", width=300, height=300)\n",
    "\n",
    "    # take 5 denoising steps, evenly from all steps\n",
    "    steps_to_take = [0, 900, 999]\n",
    "    # take 5 denoising steps from all steps\n",
    "    plot_images = []\n",
    "    for i, step in enumerate(steps_to_take):\n",
    "        val = all_denoized_unclamp[0][step]\n",
    "        img = np.array(val.cpu().detach().numpy()).reshape(28, 28)\n",
    "        image_plot = hv.Image(img, bounds=(0, 0, 28, 28))\n",
    "        image_plot.opts(\n",
    "            width=150, \n",
    "            height=150, \n",
    "            cmap='gray', \n",
    "            toolbar=None,\n",
    "            xticks=None,  # Remove x-axis ticks\n",
    "            yticks=None,  # Remove y-axis ticks\n",
    "            xaxis=None,   # Remove x-axis line and labels\n",
    "            yaxis=None    # Remove y-axis line and labels\n",
    "        )\n",
    "        text_image_step = pn.pane.Markdown(\n",
    "             f\"<span style='font-family: \\\"Times New Roman\\\"; font-size: 32px; width: 100%; text-align: center; display: block;'>Step {step}</span>\",\n",
    "            align='center')\n",
    "        plot_images.append(pn.Column(\n",
    "            image_plot, \n",
    "            text_image_step\n",
    "        ))\n",
    "        \n",
    "    # for 3 rows do arrows\n",
    "    # which indicates mu band\n",
    "    arrow_left = pn.widgets.ButtonIcon(icon='arrow-left', size='4em')\n",
    "    if label == 0:\n",
    "        arrow_left_height = -25 # top\n",
    "    elif label < 784 - 1:\n",
    "        arrow_left_height = 25 # middle\n",
    "    else:\n",
    "        arrow_left_height = 75 # bottom\n",
    "    arrow_left_height = arrow_left_height + 75 # respect margin\n",
    "        \n",
    "    label_style = {\n",
    "                'position': 'absolute',\n",
    "                'top': '-120px',\n",
    "                'left': '-16px',\n",
    "                'z-index': '1000',\n",
    "                'color': 'black',\n",
    "                'font-size': '48px',\n",
    "                'font-family': 'Times New Roman',\n",
    "                'font-weight': 'bold',\n",
    "    }\n",
    "    \n",
    "    divider_style = {\n",
    "        'width': '1px',\n",
    "        'background-color': 'black',\n",
    "        'position': 'fixed',\n",
    "    }\n",
    "        \n",
    "    label_a = pn.pane.Markdown(\n",
    "            f\"**A{index})**\", \n",
    "            styles=label_style\n",
    "    )\n",
    "    label_b = pn.pane.Markdown(\n",
    "        f\"**B{index})**\",\n",
    "        styles=label_style\n",
    "    )\n",
    "    label_c = pn.pane.Markdown(\n",
    "       f\"**C{index})**\",\n",
    "        styles=label_style\n",
    "    )\n",
    "        \n",
    "    panel_vis = pn.panel(\n",
    "        pn.Row(\n",
    "            pn.Column(\n",
    "                label_a,\n",
    "                scatter,\n",
    "                margin=(50,50,50,50)\n",
    "            ),\n",
    "            # column with vertical straight\n",
    "            pn.Column(\n",
    "                pn.pane.HTML(styles=divider_style, width=1, sizing_mode=\"stretch_height\")\n",
    "            ),\n",
    "            pn.Column(\n",
    "                label_b,\n",
    "                pn.Spacer(height=50),\n",
    "                pn.Row(*squares),\n",
    "                pn.Row(arrow_down),\n",
    "                pn.Column(*squares_flatten),\n",
    "                # text_simple_visualization,\n",
    "                pn.Spacer(height=50),\n",
    "                width=350,\n",
    "                height=350,\n",
    "                # styles={'background': 'lightgrey'},\n",
    "                margin=(50,50,50,50)\n",
    "            ),\n",
    "            pn.Column(\n",
    "                pn.pane.HTML(styles=divider_style, width=1, sizing_mode=\"stretch_height\")\n",
    "            ),\n",
    "            pn.Column(\n",
    "                label_c,\n",
    "                pn.Row(*plot_images),\n",
    "                margin=(50,50,50,50)\n",
    "            ),\n",
    "            pn.Column(\n",
    "                pn.Spacer(height=arrow_left_height),\n",
    "                arrow_left,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # show panel instance in jupyter notebook\n",
    "    return panel_vis\n",
    "panel_all = pn.Column(\n",
    "    get_paper_denoising_vis(0,1),\n",
    "    get_paper_denoising_vis(784//2,2),\n",
    "    get_paper_denoising_vis(784-1,3)\n",
    ")\n",
    "panel_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot two chromos, him-4(u924) chromosome with experimentally detected misrepresented token adn sparse synthetic generated gausian chromosomical data\n",
    "%matplotlib inline\n",
    "\n",
    "gausians = chromo_data.gausians\n",
    "not_gausians = chromo_data.not_gausians\n",
    "\n",
    "\n",
    "sns.set_theme(style='white', font_scale=4.5)\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "# create 2 axes figure\n",
    "fig, ax = plt.subplots(1, 2, figsize=(40, 10))\n",
    "ax_g54 = ax[0]\n",
    "ax_sd = ax[1]\n",
    "\n",
    "#G54_x\n",
    "g54_x = [g for g in gausians if g.g_number == 'G54' and g.chrom_label == 'X'][0]\n",
    "chromo_data = g54_x.array\n",
    "mu = g54_x.m_index\n",
    "# set plot size\n",
    "sns.scatterplot(data=chromo_data, x='POS', y='Gxx_ratio', color=\"dodgerblue\", ax = ax_g54)\n",
    "# put red line on index mu\n",
    "ax_g54.axvline(x=chromo_data.iloc[mu]['POS'], color='red', linestyle='--', ymin=0.7, ymax=0.95, dashes=(5, 5), linewidth=4)\n",
    "ax_g54.set_title('$\\it{him-4(u924)}$')\n",
    "ax_g54.set_xlabel('Physical Position (Mb)')\n",
    "ax_g54.set_ylabel('Hawaiian/Total Ratio')\n",
    "ax_g54.set_yticks([0, 0.25, 0.5, 0.75, 1])\n",
    "\n",
    "# set bold A) on a first plot\n",
    "\n",
    "ax_g54.text(-0.15, 1.1, 'A)',\n",
    "            transform=ax_g54.transAxes,\n",
    "            fontsize=80,\n",
    "            fontweight='bold',\n",
    "            fontfamily='Times New Roman')\n",
    "\n",
    "# Set x-ticks quantization\n",
    "def quantize_ticks(x, pos):\n",
    "    return f'{x // 1e6:.0f}'\n",
    "\n",
    "ax_g54.xaxis.set_major_formatter(FuncFormatter(quantize_ticks))\n",
    "\n",
    "#SD \n",
    "sd_df= chromo_dfs[len(chromo_dfs)//2-1]\n",
    "# clamp sd_df to 0, 1\n",
    "sd_df['Gxx_ratio_clamp'] = sd_df['Gxx_ratio'].clip(0, 1)\n",
    "sns.scatterplot(data=sd_df, x=sd_df.index, y='Gxx_ratio_clamp', color='dodgerblue', ax=ax_sd)\n",
    "ax_sd.set_xlabel('Element Position')\n",
    "ax_sd.set_ylabel('')\n",
    "ax_sd.set_yticks([0, 0.25, 0.5, 0.75, 1])\n",
    "\n",
    "# set bold B) on a second plot\n",
    "\n",
    "ax_sd.text(-0.15, 1.1, 'B)',\n",
    "            transform=ax_sd.transAxes,\n",
    "            fontsize=80,\n",
    "            fontweight='bold',\n",
    "            fontfamily='Times New Roman')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('plotly')\n",
    "save_to_paper = False\n",
    "\n",
    "g_54_x = [g for g in gausians if g.g_number == 'G54' and g.chrom_label == 'X'][0]\n",
    "g_54_v = [g for g in not_gausians if g.g_number == 'G54' and g.chrom_label == 'V'][0]\n",
    "chromo_data_points = g_54_v.array\n",
    "\n",
    "\n",
    "def get_brooklyn_layer(grid_size, patch_count, scale_factor, height, rescale_side=100, debug=False):\n",
    "    # Calculate patch size based on grid_size and patch_count\n",
    "    patch_size = grid_size // patch_count\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Grid size: {grid_size}, Patch count: {patch_count}, Patch size: {patch_size}\")\n",
    "\n",
    "    # Create grid\n",
    "    x, y = np.meshgrid(np.arange(grid_size), np.arange(grid_size))\n",
    "\n",
    "    x_transformed = x.copy().astype(float)\n",
    "    y_transformed = y.copy().astype(float)\n",
    "    patches = []\n",
    "\n",
    "    # Extract patches\n",
    "    for i in range(patch_count):\n",
    "        for j in range(patch_count):\n",
    "            # Calculate patch boundaries\n",
    "            x_start = i * patch_size\n",
    "            x_end = (i + 1) * patch_size\n",
    "            y_start = j * patch_size\n",
    "            y_end = (j + 1) * patch_size\n",
    "            \n",
    "            # Calculate patch center\n",
    "            center_x = (x_start + x_end) / 2\n",
    "            center_y = (y_start + y_end) / 2\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Patch {i}, {j}\")\n",
    "                print(f\"x_start: {x_start}, x_end: {x_end}, y_start: {y_start}, y_end: {y_end}\")\n",
    "                print(f\"Center x: {center_x}, Center y: {center_y}\")\n",
    "                \n",
    "            # Get patch points\n",
    "            patch_x = x[y_start:y_end, x_start:x_end]\n",
    "            patch_y = y[y_start:y_end, x_start:x_end]\n",
    "            \n",
    "            # Scale points toward center\n",
    "            x_transformed[y_start:y_end, x_start:x_end] = center_x + (patch_x - center_x) * scale_factor\n",
    "            y_transformed[y_start:y_end, x_start:x_end] = center_y + (patch_y - center_y) * scale_factor\n",
    "            \n",
    "            patches.append((center_x, center_y))\n",
    "\n",
    "        # Create z-coordinates for the two layers\n",
    "        z_transformed = np.ones_like(x.flatten()) * height\n",
    "        \n",
    "    # rescale x_transformed and y_transformed to [0, rescale_side]\n",
    "    x_transformed = np.interp(x_transformed, (0, grid_size), (0, rescale_side))\n",
    "    y_transformed = np.interp(y_transformed, (0, grid_size), (0, rescale_side))\n",
    "        \n",
    "    return x_transformed, y_transformed, z_transformed, patches\n",
    "\n",
    "def get_brooklyn_3d_plot(\n",
    "        chromo_data_points,  \n",
    "        dot_from_x = 106,\n",
    "        dot_from_y = 6,\n",
    "        dot_from_z = 0.35,\n",
    "        show_text_plane = False,\n",
    "        misrepresented_token_position = None,\n",
    "        show_bulbs = True,\n",
    "    ):\n",
    "    const_measurement_per_point_cloud = 200\n",
    "    len_chromo_data = len(chromo_data_points['POS'])\n",
    "    num_of_point_clouds = len_chromo_data // const_measurement_per_point_cloud\n",
    "    side_min = math.ceil(math.sqrt(num_of_point_clouds))\n",
    "    side_max = math.ceil(math.sqrt(len_chromo_data))\n",
    "    points_in_section = (side_max//side_min)**2\n",
    "\n",
    "    # adjust side_max so it is divisible by side_min\n",
    "    side_max = side_min * (side_max // side_min)\n",
    "    print(f\"Side min: {side_min}, Side max: {side_max}, Num of point clouds: {num_of_point_clouds}, chromo data len: {len_chromo_data}, Points in section: {points_in_section}\")\n",
    "\n",
    "    layers = []\n",
    "    # floor level\n",
    "    floor = get_brooklyn_layer(grid_size=side_max, patch_count=side_min, scale_factor=0.8, height=1.0, rescale_side=side_max)\n",
    "    # floor power\n",
    "    floor_power_idx = np.linspace(0, len(chromo_data_points)-1, side_max**2, dtype=int)\n",
    "    floor_power = chromo_data_points['Gxx_ratio'].iloc[floor_power_idx].to_numpy()\n",
    "    floor_color = floor_power.copy()\n",
    "    floor_color = floor_color / 2.2 # rescale to 0 - 0.25\n",
    "    # modify floor_z slightly by multiplying by power each element by each element\n",
    "    floor_z = floor[2] * floor_power * 0.3 + 0.2\n",
    "    print(f\"Floor z: {floor_z}\")\n",
    "    floor_x = floor[0]\n",
    "    floor_y = floor[1]\n",
    "    \n",
    "    if misrepresented_token_position is not None:\n",
    "        # get index of closes POS to misrepresented token\n",
    "        closest_idx = np.argmin(np.abs(chromo_data_points['POS'] - misrepresented_token_position))\n",
    "        # get its relative position\n",
    "        relative_pos = closest_idx/len_chromo_data\n",
    "        print(f\"Closest idx: {closest_idx}, Relative pos: {relative_pos}\")\n",
    "        # detect in which point cloud is misrepresented token\n",
    "        cell_idx = int(relative_pos * side_min**2) - 1 # not sure why -1\n",
    "        point_cloud_idx = int(relative_pos * len(floor_z))  \n",
    "        print(f\"Closest idx: {closest_idx}, Cell idx: {cell_idx}, Point cloud idx: {point_cloud_idx}\")\n",
    "        cell_x = cell_idx % side_min\n",
    "        cell_y = cell_idx // side_min\n",
    "        print(f\"Cell x: {cell_x}, Cell y: {cell_y}\")\n",
    "        cell_x_start = cell_x * (side_max // side_min)\n",
    "        cell_y_start = cell_y * (side_max // side_min)\n",
    "        cell_x_end = cell_x_start + (side_max // side_min)\n",
    "        cell_y_end = cell_y_start + (side_max // side_min)\n",
    "        # get exact position of misrepresented token in point cloud\n",
    "        misrepresented_token_position_x = point_cloud_idx % points_in_section\n",
    "        misrepresented_token_position_y = point_cloud_idx // points_in_section\n",
    "        # power of misrepresented token x10\n",
    "        floor_power[point_cloud_idx] = 2\n",
    "        # set color to max\n",
    "        floor_color[point_cloud_idx] = 1\n",
    "        # set lines to point_cloud_idx point\n",
    "        dot_from_x = floor_x.flatten()[point_cloud_idx]\n",
    "        dot_from_y = floor_y.flatten()[point_cloud_idx]\n",
    "        dot_from_z = floor_z[point_cloud_idx]\n",
    "        # scale x,y,z around dot_from_x, dot_from_y, dot_from_z\n",
    "        scale_Factor= 2\n",
    "        for i in range(len(floor_x)):\n",
    "            floor_x[i] = dot_from_x + (floor_x[i] - dot_from_x) * scale_Factor\n",
    "            floor_y[i] = dot_from_y + (floor_y[i] - dot_from_y) * scale_Factor\n",
    "            floor_z[i] = dot_from_z + (floor_z[i] - dot_from_z) * scale_Factor\n",
    "        # put Z of it upper for 0.05\n",
    "        floor_z[point_cloud_idx] = floor_z[point_cloud_idx] + 0.05\n",
    "        print(f\"Misrepresented token position x: {misrepresented_token_position_x}, Misrepresented token position y: {misrepresented_token_position_y}\")\n",
    "        # for each point in cell set z to 0.5\n",
    "        # reshape floor_z to [side_max, side_max]\n",
    "        floor_z = floor_z.reshape(side_max, side_max)\n",
    "        \n",
    "        ## visible neighborhood \n",
    "        # z_non_mtp_delta = np.max(floor_z) / 4\n",
    "        # floor_z[0:cell_y_start] = floor_z[0:cell_y_start] - z_non_mtp_delta\n",
    "        # floor_z[cell_y_end:] = floor_z[cell_y_end:] - z_non_mtp_delta\n",
    "        \n",
    "        # floor_z[:, 0:cell_x_start] = floor_z[:, 0:cell_x_start] - z_non_mtp_delta\n",
    "        # floor_z[:, cell_x_end:] =  floor_z[:, cell_x_end:] - z_non_mtp_delta\n",
    "        \n",
    "        # not visible neighborhood\n",
    "        z_non_mtp = 1000\n",
    "        floor_z[0:cell_y_start] = z_non_mtp\n",
    "        floor_z[cell_y_end:] = z_non_mtp\n",
    "        \n",
    "        floor_z[:, 0:cell_x_start] = z_non_mtp\n",
    "        floor_z[:, cell_x_end:] = z_non_mtp\n",
    "        \n",
    "        print(f\"Cell x start: {cell_x_start}, Cell x end: {cell_x_end}, Cell y start: {cell_y_start}, Cell y end: {cell_y_end}\")\n",
    "        floor_z = floor_z.flatten()\n",
    "    \n",
    "    layers.append((floor_x, floor_y, floor_z, floor_color, floor_power * 0.5))\n",
    "\n",
    "    # bulb level\n",
    "    bulbs = get_brooklyn_layer(grid_size=side_min, patch_count=side_min, scale_factor=0, height=2.0, rescale_side=side_max)\n",
    "    bulbs_power_idx = np.linspace(0, len(chromo_data_points)-1, side_min**2, dtype=int)\n",
    "    bulbs_power = chromo_data_points['Gxx_ratio'].iloc[bulbs_power_idx].to_numpy()\n",
    "    bulbs_color = bulbs_power.copy()\n",
    "    # rescale color t0 0.5 - 1\n",
    "\n",
    "    bulbs_z = bulbs[2] * bulbs_power * 0.5 + 0.1\n",
    "    bulbs_x = bulbs[0]\n",
    "    bulbs_y = bulbs[1]\n",
    "    bulbs_power = bulbs_power * 2\n",
    "    bulbs_color = bulbs_color / 2 + 0.5 # rescale to 0.5 - 1\n",
    "\n",
    "    if not show_bulbs:\n",
    "        bulbs_z = bulbs_z * 10000\n",
    "        \n",
    "    layers.append((bulbs_x, bulbs_y, bulbs_z, bulbs_color, bulbs_power * 2))\n",
    "\n",
    "    x = [x.flatten() for x, _, _, _, _ in layers]\n",
    "    y = [y.flatten() for _, y, _ , _, _ in layers]\n",
    "    z = [z for _, _, z, _, _ in layers]\n",
    "    c = [c for _, _, _, c, _ in layers]\n",
    "    s = [s for _, _, _, _, s in layers]\n",
    "\n",
    "    x = np.concatenate(x)\n",
    "    y = np.concatenate(y)\n",
    "    z = np.concatenate(z)\n",
    "    c = np.concatenate(c)\n",
    "    s = np.concatenate(s)\n",
    "    \n",
    "    print(f\"X: {x.shape}, Y: {y.shape}, Z: {z.shape}, C: {c.shape}, S: {s.shape}\")\n",
    "    df = pd.DataFrame({'x': x, 'y': y, 'z': z, 'color': c, 'size': s})\n",
    "    print(f\"Dataframe shapes: {df.shape}\")\n",
    "    # Create 3D scatter plot    \n",
    "    fig = px.scatter_3d(\n",
    "        df, \n",
    "        x='x', \n",
    "        y='y', \n",
    "        z='z', \n",
    "        color='color', \n",
    "        size='size',\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        marker=dict(\n",
    "            line=dict(width = 1), \n",
    "            opacity = 1.0,\n",
    "        )\n",
    "    )  # Remove white outline\n",
    "\n",
    "    if show_text_plane:\n",
    "        # create 31x31 random ATGC letters array\n",
    "        # each point cloud have 32x32=1024 points\n",
    "        atgc_squared_size = 32\n",
    "        letters = np.random.choice(['A', 'T', 'G', 'C'], (atgc_squared_size, atgc_squared_size))\n",
    "        char_mat = {\n",
    "            'A': 'A',\n",
    "            'T': 'T',\n",
    "            'G': 'G',\n",
    "            'C': 'C',\n",
    "            'M': '<b>A</b>'\n",
    "        }\n",
    "        colors_map = {\n",
    "            'A': 'black',\n",
    "            'T': 'black',\n",
    "            'G': 'black',\n",
    "            'C': 'black',\n",
    "            'M': 'red'\n",
    "        }\n",
    "        size_map = {\n",
    "            'A': 7,\n",
    "            'T': 7,\n",
    "            'G': 7,\n",
    "            'C': 7,\n",
    "            'M': 7\n",
    "        }\n",
    "        print(f\"Letters shape: {letters.shape}\")\n",
    "        for i in range(len(letters)):\n",
    "            row = letters[i]\n",
    "            # put diamond on the middle of the array\n",
    "            if misrepresented_token_position is not None and i == len(letters) // 2:\n",
    "                row[len(row)//2] = 'M'\n",
    "                \n",
    "        # Create grid of points for text\n",
    "\n",
    "        x_start_letters = side_max / 2\n",
    "        y_start_letters = side_max / 2\n",
    "        x_end_letters = side_max - 1\n",
    "        y_end_letters = side_max - 1\n",
    "\n",
    "        x_points = np.linspace(x_start_letters, x_end_letters, atgc_squared_size)\n",
    "        y_points = np.linspace(y_start_letters, y_end_letters, atgc_squared_size)\n",
    "        X, Y = np.meshgrid(x_points, y_points)\n",
    "\n",
    "        text_x = X.flatten()\n",
    "        text_y = Y.flatten()\n",
    "\n",
    "        # lines connecting letters to research point\n",
    "        line_width = 2\n",
    "        line_1 = go.Scatter3d(\n",
    "            x=[dot_from_x, x_start_letters],\n",
    "            y=[dot_from_y, y_start_letters],\n",
    "            z=[dot_from_z, 0.1],\n",
    "            mode='lines',\n",
    "            line=dict(color='black', width=line_width),\n",
    "            showlegend=False\n",
    "        )\n",
    "        line_2 = go.Scatter3d(\n",
    "            x=[dot_from_x, x_end_letters],\n",
    "            y=[dot_from_y, y_start_letters],\n",
    "            z=[dot_from_z, 0.1],\n",
    "            mode='lines',\n",
    "            line=dict(color='black', width=line_width),\n",
    "            showlegend=False\n",
    "        )\n",
    "        line_3 = go.Scatter3d(\n",
    "            x=[dot_from_x, x_start_letters],\n",
    "            y=[dot_from_y, y_end_letters],\n",
    "            z=[dot_from_z, 0.1],\n",
    "            mode='lines',\n",
    "            line=dict(color='black', width=line_width),\n",
    "            showlegend=False\n",
    "        )\n",
    "        line_4 = go.Scatter3d(\n",
    "            x=[dot_from_x, x_end_letters],\n",
    "            y=[dot_from_y, y_end_letters],\n",
    "            z=[dot_from_z, 0.1],\n",
    "            mode='lines',\n",
    "            line=dict(color='black', width=line_width),\n",
    "            showlegend=False\n",
    "        )\n",
    "\n",
    "\n",
    "        text_trace = go.Scatter3d(\n",
    "            x=text_x,\n",
    "            y=text_y,\n",
    "            z=np.full(X.size, 0.01),  # Slightly above surface\n",
    "            mode='text',\n",
    "            text=[char_mat[letter] for row in letters for letter in row],\n",
    "            textfont=dict(\n",
    "                size=[size_map[letter] for row in letters for letter in row],\n",
    "                family='Courier New, monospace',\n",
    "                color=[colors_map[letter] for row in letters for letter in row]\n",
    "            ),\n",
    "            showlegend=False\n",
    "        )\n",
    "\n",
    "        # fig.add_trace(surface_plane)\n",
    "        fig.add_trace(text_trace)\n",
    "\n",
    "        # Add lines\n",
    "        fig.add_trace(line_1)\n",
    "        fig.add_trace(line_2)\n",
    "        fig.add_trace(line_3)\n",
    "        fig.add_trace(line_4)\n",
    "        \n",
    "\n",
    "    fig.update_layout(\n",
    "    scene=dict(\n",
    "        camera=dict(\n",
    "            eye=dict(x=1.85, y=0.99, z=1.1),\n",
    "        ),\n",
    "        # Ensure axes ranges are appropriate\n",
    "    )\n",
    "    )\n",
    "\n",
    "    last_color = 'orange'\n",
    "    if misrepresented_token_position is not None:\n",
    "        last_color = 'red'\n",
    "\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            zaxis=dict(range=[0, 2], showgrid=False, showbackground=False, showticklabels=False, title=''),\n",
    "            xaxis=dict(range=[0, side_max], showgrid=False, showbackground=False, showticklabels=False, title=''),\n",
    "            yaxis=dict(range=[0, side_max], showgrid=False, showbackground=False, showticklabels=False, title='')\n",
    "        ),\n",
    "        width=800, \n",
    "        height=800,\n",
    "        coloraxis=dict(colorscale=[[0, 'black'], [0.25, 'black'],  [0.5, 'yellow'], [0.55, 'purple'], [0.85, 'purple'], [1, last_color]]),\n",
    "        showlegend=False,\n",
    "        coloraxis_showscale=False\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def save_brooklyn_plot(fig):\n",
    "    img_path = \"./brooklyn_plot.png\"\n",
    "    pio.orca.config.use_xvfb = True\n",
    "    fig.write_image(img_path, width=800, height=800, scale=4, format='png', engine=\"orca\")\n",
    "    # crop white edges\n",
    "    im = Image.open(img_path)\n",
    "    im_rgb = im.convert('RGB')\n",
    "    bg = Image.new(im_rgb.mode, im.size, im.getpixel((0,0)))\n",
    "    diff = ImageChops.difference(im_rgb, bg)\n",
    "    diff = ImageChops.add(diff, diff, 2.0, -100)\n",
    "    bbox = diff.getbbox()\n",
    "    if bbox:\n",
    "        print(f\"Cropping image to bbox: {bbox}\")\n",
    "        img = im.crop(bbox)\n",
    "    else:\n",
    "        print(\"No bbox found\")\n",
    "    # remove old image\n",
    "    os.remove(img_path)\n",
    "    return img\n",
    "    \n",
    "# Add custom colorbar legend with parametrized dimensions\n",
    "def add_colorbar_legend(fig, \n",
    "                       # Position parameters\n",
    "                       x_pos=0,           # x position of the colorbar\n",
    "                       y_min=0.05,        # bottom of colorbar\n",
    "                       y_max=0.25,        # top of colorbar\n",
    "                       width=0.015,       # width of the colorbar\n",
    "                       label_offset=0.02,  # distance of labels from colorbar\n",
    "                       # Style parameters\n",
    "                       num_steps=1000,    # number of rectangles for gradient\n",
    "                       border_width=1,    # width of the border\n",
    "                       font_size=12):     # size of labels\n",
    "    \n",
    "    # Calculate steps for gradient\n",
    "    y_steps = np.linspace(y_min, y_max, num_steps)\n",
    "    step_height = (y_max - y_min) / num_steps\n",
    "    \n",
    "    # Create shapes list for all the rectangles\n",
    "    shapes = []\n",
    "    \n",
    "    # Helper function to get color for each position in the gradient\n",
    "    def get_color(pos):\n",
    "        # Interpolate from black to yellow over the full range\n",
    "        f = pos\n",
    "        return f'rgb({int(255*f)},{int(255*f)},0)'\n",
    "    \n",
    "    # Create gradient rectangles\n",
    "    for i in range(num_steps):\n",
    "        pos = i / (num_steps - 1)\n",
    "        shapes.append(dict(\n",
    "            type='rect',\n",
    "            x0=x_pos,\n",
    "            x1=x_pos + width,\n",
    "            y0=y_steps[i],\n",
    "            y1=y_steps[i] + step_height,\n",
    "            xref='paper',\n",
    "            yref='paper',\n",
    "            fillcolor=get_color(pos),\n",
    "            line=dict(width=0),\n",
    "        ))\n",
    "    \n",
    "    # Add border rectangle\n",
    "    shapes.append(dict(\n",
    "        type='rect',\n",
    "        x0=x_pos,\n",
    "        x1=x_pos + width,\n",
    "        y0=y_min,\n",
    "        y1=y_max,\n",
    "        xref='paper',\n",
    "        yref='paper',\n",
    "        fillcolor='rgba(0,0,0,0)',\n",
    "        line=dict(color='black', width=border_width),\n",
    "    ))\n",
    "    \n",
    "    # Update the layout with the new shapes and annotations\n",
    "    fig.update_layout(\n",
    "        annotations=[\n",
    "            dict(\n",
    "                x=x_pos + width + label_offset,\n",
    "                y=y_max,\n",
    "                xanchor='left',\n",
    "                yanchor='bottom',\n",
    "                text='1.0',\n",
    "                showarrow=False,\n",
    "                font=dict(size=font_size, color='black')\n",
    "            ),\n",
    "            dict(\n",
    "                x=x_pos + width + label_offset,\n",
    "                y=y_min,\n",
    "                xanchor='left',\n",
    "                yanchor='bottom',\n",
    "                text='0.0',\n",
    "                showarrow=False,\n",
    "                font=dict(size=font_size, color='black')\n",
    "            )\n",
    "        ],\n",
    "        shapes=shapes\n",
    "    )\n",
    "\n",
    "\n",
    "fig_g_54_v = get_brooklyn_3d_plot(g_54_v.array, show_text_plane=False)\n",
    "fig_g_54_x = get_brooklyn_3d_plot(g_54_x.array, show_text_plane=False)\n",
    "fig_g_54_x_focused = get_brooklyn_3d_plot(g_54_x.array, misrepresented_token_position=9742980, show_bulbs=False, show_text_plane=True)\n",
    "\n",
    "print(\"Adding colorbar legend to G54 X\")\n",
    "add_colorbar_legend(\n",
    "        fig=fig_g_54_x,\n",
    "        x_pos=0,\n",
    "        y_min=0.05,\n",
    "        y_max=0.45,\n",
    "        width=0.025,\n",
    "        label_offset=0.02,\n",
    "        num_steps=10000,\n",
    "        border_width=1,\n",
    "        font_size=12\n",
    ")\n",
    "print(\"Added colorbar legend to G54 X\")\n",
    "\n",
    "plots = [\n",
    "    save_brooklyn_plot(fig_g_54_x),\n",
    "    save_brooklyn_plot(fig_g_54_x_focused),\n",
    "    save_brooklyn_plot(fig_g_54_v)\n",
    "]\n",
    "\n",
    "# thumbnail images to min height\n",
    "min_height = min([img.height for img in plots])\n",
    "min_width = min([img.width for img in plots])\n",
    "min_dim = min(min_height, min_width)\n",
    "[img.thumbnail((min_dim, min_dim)) for img in plots]\n",
    "\n",
    "# concatenate images\n",
    "cell_width = plots[0].width\n",
    "image_grid = Image.new('RGB', (min_dim * 3, min_dim), color='white')\n",
    "for i, img in enumerate(plots):\n",
    "    image_grid.paste(img, (i * cell_width, 0))\n",
    "# add 50 px margin\n",
    "image_grid = ImageOps.expand(image_grid, border=50, fill='white')\n",
    "image_grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chromo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
