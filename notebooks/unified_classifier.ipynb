{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import torch \n",
    "import gc\n",
    "import plotly.graph_objects as go\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.v2 import PILToTensor,Compose\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import re\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange \n",
    "import yaml\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt \n",
    "from functools import partial, reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "import panel as pn\n",
    "\n",
    "import holoviews as hv\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from yellowbrick.text import TSNEVisualizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from holoviews import dim, opts\n",
    "from dataclasses import dataclass\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from src.md import MDDataset, generate_phases_for_dense_validation, MDDenseSet, MDLoadable, MDDense, MDDensePhaseData\n",
    "from src.models.denoiser import DenoiserModelPipeline, DenoiserModel, get_forward_diffusion_params, forward_add_noise\n",
    "from src.preprocessing import ChromoDataContainer, ChromoData\n",
    "from src.models.unified_classifier import evaluate_test_set, evaluate_unified_classifier_model, predictCLS, ClassifierModelPipeline\n",
    "\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for unified classifier model\n",
    "def train_unified_classifier_model(\n",
    "    dataset,\n",
    "    model,\n",
    "    run_name = 'unnamed_unified_classifier_model',\n",
    "    n_epochs = 100,\n",
    "    batch_size = 32,\n",
    "    \n",
    "    timestep = 1, # fixed timestep for the base denoiser model\n",
    "    label_num = 5, # fixed label num for the base denoiser model\n",
    "    label_count = 10, # fixed label count for the base denoiser model\n",
    "    \n",
    "    lr=10e-3,\n",
    "    validation_portion=0.1,\n",
    "    validation_per_epoch = 0,\n",
    "    validation_samples = 1024,\n",
    "    \n",
    "    distilled_val_sets = None, # TODO: refactor this to have better generalization\n",
    "    \n",
    "    checkpoints_folder='../checkpoints/classifier'\n",
    "):\n",
    "    T, betas, alphas, alphas_cumprod, alphas_cumprod_prev, variance = get_forward_diffusion_params()\n",
    "    \n",
    "    alphas_cumprod = alphas_cumprod.to(device)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    print(\"Freezing denoiser_model\")\n",
    "    # # freeze all parameters except classification head\n",
    "    for name, param in model.denoiser_model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "        # print(f\"Freezing: {name}\")\n",
    "    \n",
    "    print(\"Unfreezing classifier_head\")\n",
    "    # Ensure the classification head parameters require gradients\n",
    "    for name, param in model.classifier_head.named_parameters():\n",
    "        param.requires_grad = True\n",
    "        # print(f\"Unfreezing: {name}\")\n",
    "\n",
    "    # loss for classification\n",
    "    loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "    # optimizer for classification head only\n",
    "    optimzer=torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr)\n",
    "    \n",
    "    # define scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimzer, step_size=10, gamma=0.9)\n",
    "    train_ds, val_ds = dataset.split(test_size=validation_portion)\n",
    "    \n",
    "    # creating directory for checkpoints\n",
    "    os.makedirs(checkpoints_folder, exist_ok=True)\n",
    "    \n",
    "    # setting checkpoint paths\n",
    "    path_checkpoint = os.path.join(checkpoints_folder, f\"{run_name}.pth\")\n",
    "\n",
    "\n",
    "    dataloader=DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last = True,\n",
    "        num_workers=10,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    dataloader_eval = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last = True,\n",
    "        num_workers=10,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    timestamp_int = int(datetime.now().timestamp())\n",
    "    log_name = f\"{timestamp_int}_{run_name}\"\n",
    "    print(f\"Log name: {log_name}\")\n",
    "    writer = SummaryWriter(log_dir=f'../logs/{log_name}')\n",
    "\n",
    "    iter_count=0\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        epoch_iter = 0\n",
    "        \n",
    "        if epoch % validation_per_epoch == 0:\n",
    "            if epoch > 0:\n",
    "                torch.save(model.state_dict(),path_checkpoint)\n",
    "                print(\"Model saved\")\n",
    "            print(\"Validating\")\n",
    "            \n",
    "            # VALIDATION\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # validation on random samples from validation set\n",
    "                \n",
    "                print(\"Validation on val samples\")\n",
    "                result_val = evaluate_test_set(model, predictCLS, dataloader_eval, alphas_cumprod, timestep, label_num, label_count, iterations=validation_samples // batch_size)\n",
    "                writer.add_scalar('Error/val', result_val['error_percentage'], iter_count)\n",
    "                \n",
    "                \n",
    "                # TODO: refactor this to have better generalization\n",
    "                # validation on RD\n",
    "                \n",
    "                result_rel_mu, result_rel_lowess, result_gen_mu, result_gen_lowess = evaluate_unified_classifier_model(\n",
    "                    model,\n",
    "                    distilled_val_sets,\n",
    "                    alphas_cumprod,\n",
    "                    timestep,\n",
    "                    label_num,\n",
    "                    label_count,\n",
    "                    batch_size,\n",
    "                    predictCLS  \n",
    "                )\n",
    "                \n",
    "                writer.add_scalar('Error/real_mu', result_rel_mu['total_abs_percentage_error'], iter_count)\n",
    "                writer.add_scalar('Error/real_lowess', result_rel_lowess['total_abs_percentage_error'], iter_count)\n",
    "                writer.add_scalar('Error/gen_mu', result_gen_mu['total_abs_percentage_error'], iter_count)\n",
    "                writer.add_scalar('Error/gen_lowess', result_gen_lowess['total_abs_percentage_error'], iter_count)\n",
    "                \n",
    "                # switch back to train\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            print(\"Validation done\")\n",
    "        \n",
    "        for datachunk in dataloader:\n",
    "            \n",
    "            gxx = datachunk['gxx']\n",
    "            labels_classifier = datachunk['labels_classifier']\n",
    "            \n",
    "            gxx=gxx.to(device)\n",
    "            labels_classifier=labels_classifier.to(device)\n",
    "            \n",
    "            # fixed conditions\n",
    "            # t=torch.randint(0,TIMESTEP,(BATCH_SIZE,)).to(device)\n",
    "            t=torch.full((batch_size,),timestep,dtype=torch.long).to(device)\n",
    "            y=torch.full((batch_size,),label_num,dtype=torch.long).to(device)\n",
    "            \n",
    "            x=gxx*2-1\n",
    "            # print(x.shape)\n",
    "            x,noise=forward_add_noise(x,t, alphas_cumprod)\n",
    "            labels_pred, _=model(x,t,y)\n",
    "            \n",
    "            loss = loss_fn(labels_pred, labels_classifier)\n",
    "            \n",
    "            optimzer.zero_grad()\n",
    "            loss.backward()\n",
    "            # grad clip\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimzer.step()\n",
    "            iter_count += 1\n",
    "            epoch_iter += 1\n",
    "            \n",
    "            if iter_count % 100 == 0:\n",
    "                writer.add_scalar('Loss/train', loss, iter_count)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "            \n",
    "    print(\"Training done, saving model\")\n",
    "    torch.save(model.state_dict(), path_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TRANING OF UNIFIED MODEL\n",
    "\n",
    "# # creating val datasets for real data\n",
    "chrom_data = ChromoDataContainer.load_from_pkl()\n",
    "\n",
    "# TODO: Split MD and DenseD to separatte notebooks\n",
    "# 1) MD generates source for Dense MD\n",
    "# 2) DenseD generates Both Dense MD and Dense RD\n",
    "distiled_real_mu = generate_phases_for_dense_validation(chrom_data.gausians, do_real_mu=True)\n",
    "distiled_real_lowess = generate_phases_for_dense_validation(chrom_data.gausians, do_real_mu=False)\n",
    "distiled_mock_mu = MDLoadable.load(\n",
    "    config_name=\"dataset_128_128_784_classifier_control_set_mu\",\n",
    "    config_folder=\"../configs/MDDense\",\n",
    "    dataset_folder=\"../data/MDDense\",\n",
    "    dataset_cls=MDDenseSet)\n",
    "distiled_mock_lowess = MDLoadable.load(\n",
    "    config_name=\"dataset_128_128_784_classifier_control_set_lowess\",\n",
    "    config_folder=\"../configs/MDDense\",\n",
    "    dataset_folder=\"../data/MDDense\",\n",
    "    dataset_cls=MDDenseSet)\n",
    "\n",
    "\n",
    "# # Automated trainer\n",
    "\n",
    "configs_dir = '../configs/classifier'\n",
    "configs = os.listdir(configs_dir)\n",
    "\n",
    "print(f\"Found {len(configs)} configs\")\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    \n",
    "pipelines = []\n",
    "\n",
    "for config in tqdm(configs):\n",
    "    print(\"=====================================\")\n",
    "    classifierModelPipeline = ClassifierModelPipeline.load(config, skip_data_load=True)\n",
    "    if classifierModelPipeline.config.training.skip:\n",
    "        print(f\"Skipping training for {config}\")\n",
    "        continue\n",
    "    pipelines.append(classifierModelPipeline)\n",
    "    print(f\"Loaded model {classifierModelPipeline.config.model.description}\")\n",
    "    print(f\"Loaded model {config}\")\n",
    "    \n",
    "print(\"Done loading models\")\n",
    "\n",
    "# Loading training datasets\n",
    "\n",
    "for pipeline in pipelines:\n",
    "    pipeline.load_dataset()\n",
    "    \n",
    "print(\"Done loading datasets\")\n",
    "\n",
    "# Training models\n",
    "\n",
    "for pipeline in pipelines:\n",
    "    print(\"=====================================\")\n",
    "    print(f\"Training model {pipeline.config.model.description}\")\n",
    "    print(f\"Training params:\")\n",
    "    print(pipeline.config.training)\n",
    "    train_unified_classifier_model(\n",
    "        pipeline.dataset,\n",
    "        pipeline.model,\n",
    "        run_name = pipeline.file_name,\n",
    "        n_epochs = pipeline.config.training.n_epochs,\n",
    "        lr = pipeline.config.training.learning_rate,\n",
    "        batch_size=pipeline.config.training.batch_size,\n",
    "        validation_per_epoch = pipeline.config.training.validation_per_epoch,\n",
    "        validation_portion = pipeline.config.training.validation_portion,\n",
    "        \n",
    "        label_num=pipeline.config.training.label_num,\n",
    "        timestep=pipeline.config.training.timestep,\n",
    "        label_count=pipeline.config.training.label_count,\n",
    "        \n",
    "        distilled_val_sets=(\n",
    "            distiled_real_mu, \n",
    "            distiled_real_lowess,\n",
    "            distiled_mock_mu.data, \n",
    "            distiled_mock_lowess.data\n",
    "        ) # refactor this to have better generalization\n",
    "    )\n",
    "    print(f\"Training model {pipeline.config.model.description} done\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chromo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
