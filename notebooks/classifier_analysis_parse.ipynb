{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "import math\n",
    "import sys\n",
    "\n",
    "hv.extension(\"plotly\")\n",
    "pn.extension(\"plotly\")\n",
    "pn.extension('tabulator', theme='dark')\n",
    "pn.config.theme = 'dark'\n",
    "hv.renderer('plotly').theme = 'dark'\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is used to generate the results for the paper\n",
    "# It loads the latest results from the metrics/results folder and postprocesses them into md files\n",
    "\n",
    "# get latest results from a folder\n",
    "files_results = os.listdir(\"../metrics/results\")\n",
    "files_results.sort(key=lambda x: int(x.split(\"_\")[0]))\n",
    "with open(f\"../metrics/results/{files_results[-1]}\", 'rb') as f:\n",
    "    evaluation_results_loaded = pickle.load(f)\n",
    "    \n",
    "evaluation_results = []\n",
    "    \n",
    "# postprocess\n",
    "for i, (model_name, result, config, denoiser_config) in enumerate(evaluation_results_loaded):\n",
    "    # add accuracy calculations\n",
    "    for i in range(4):\n",
    "        result[i][\"accuracy\"] = 100 - result[i][\"total_abs_percentage_error\"]\n",
    "        \n",
    "    # skip if denoiser_config have Conv kernel = 2, excluded from paper\n",
    "    if denoiser_config is not None and int(denoiser_config.model.parameters.patch_size) != 4:\n",
    "        print(f\"Skipping {model_name} due to Conv kernel = 2\")\n",
    "        continue\n",
    "    \n",
    "    # skip if denoiser config have not 768 labels, excluded from paper\n",
    "    if denoiser_config is not None and int(denoiser_config.model.parameters.label_num) != 784:\n",
    "        print(f\"Skipping {model_name} due to not 768 labels\")\n",
    "        continue\n",
    "        \n",
    "    evaluation_results.append((model_name, result, config, denoiser_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aggregated top results\n",
    "rows = []\n",
    "for model, result, unified_config, denoiser_config in evaluation_results:\n",
    "    \n",
    "    row = {\n",
    "        \"pred_real_mu\": result[0]['accuracy'],\n",
    "        \"pred_real_lowess\": result[1]['accuracy'],\n",
    "        \"pred_mock_mu\": result[2]['accuracy'],\n",
    "        \"pred_mock_lowess\": result[3]['accuracy'],\n",
    "    }\n",
    "    if unified_config:\n",
    "        row['type']=\"Unified\"\n",
    "        row['classifier'] = unified_config.model.cls_head\n",
    "        row[\"Label #\"] = int(denoiser_config.model.parameters.label_num)\n",
    "        row[\"Conv kernel\"] = int(denoiser_config.model.parameters.patch_size)\n",
    "    else:\n",
    "        row['type'] = model\n",
    "        row[\"classifier\"] = \"\"\n",
    "        row[\"Label #\"] = \"\"\n",
    "        row[\"Conv kernel\"] = \"\"\n",
    "    rows.append(row)\n",
    "dataframe_results = pd.DataFrame(rows)\n",
    "\n",
    "generate_result = lambda name, filtered_df: {\n",
    "        \"Classifier    \": name,\n",
    "        \"dense MD LOESS *mu*\": f'{filtered_df.sort_values(by=[\"pred_mock_lowess\"], ascending=False).iloc[0][\"pred_mock_lowess\"]:0.2f}%',\n",
    "        \"RD LOESS *mu*\": f'{filtered_df.sort_values(by=[\"pred_real_lowess\"], ascending=False).iloc[0][\"pred_real_lowess\"]:0.2f}%',\n",
    "        \"dense MD *mu*\": f'{filtered_df.sort_values(by=[\"pred_mock_mu\"], ascending=False).iloc[0][\"pred_mock_mu\"]:0.2f}%',\n",
    "        \"RD *mu*\": f'**{filtered_df.sort_values(by=[\"pred_real_mu\"], ascending=False).iloc[0][\"pred_real_mu\"]:0.2f}%**',\n",
    "    }\n",
    "\n",
    "results = [\n",
    "     generate_result(\n",
    "        \"Conventional\",\n",
    "        dataframe_results.loc[dataframe_results['type'] == \"Conventional separate classifier\"]),\n",
    "    generate_result(\n",
    "        \"Linear\",\n",
    "        dataframe_results.loc[(dataframe_results['type'] == \"Unified\") & \n",
    "                            (dataframe_results['classifier'] == \"linear\")]),\n",
    "    generate_result(\n",
    "        \"Attention\",\n",
    "        dataframe_results.loc[(dataframe_results['type'] == \"Unified\") & \n",
    "                            (dataframe_results['classifier'] == \"attn\")]),  \n",
    "    generate_result(\n",
    "        \"Zero-shot\",\n",
    "        dataframe_results.loc[dataframe_results['type'] == \"Zero-shot unified classifier\"]),\n",
    "]  \n",
    "        \n",
    "dataframe_top_results = pd.DataFrame(results)\n",
    "os.makedirs(\"../metrics/result_tables\", exist_ok=True)\n",
    "markdown = dataframe_top_results.to_markdown(index=False)\n",
    "with open(\"../metrics/result_tables/cls_top_results.md\", \"w\") as f:\n",
    "    f.write(markdown)\n",
    "table = pn.pane.DataFrame(dataframe_top_results, sizing_mode=\"stretch_both\", max_height=900)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEASURED RESULTS\n",
    "\n",
    "def roman_to_integer(roman_string):\n",
    "    if roman_string == \"I\":\n",
    "        return 1\n",
    "    if roman_string == \"II\":\n",
    "        return 2\n",
    "    if roman_string == \"III\":\n",
    "        return 3\n",
    "    if roman_string == \"IV\":\n",
    "        return 4\n",
    "    if roman_string == \"V\":\n",
    "        return 5\n",
    "    if roman_string == \"X\":\n",
    "        return 6\n",
    "\n",
    "rows = []\n",
    "for model, result, unified_config, denoiser_config in evaluation_results:\n",
    "    real_mu_results = result[0]['pairs_measured_calculated']\n",
    "    for result_tuple in real_mu_results:\n",
    "        \n",
    "        # filter out results with Conv kernel 2 for the paper\n",
    "        if unified_config and denoiser_config.model.parameters.patch_size == 2:\n",
    "            continue\n",
    "        \n",
    "        row = {\n",
    "            'Chrom': roman_to_integer(result_tuple[1]),\n",
    "            'RD *mu*': f\"{result_tuple[2]:,}\",\n",
    "            'Predicted *mu*': f\"{result_tuple[3]:,}\",\n",
    "            '**Delta (Megabase)**': f\"**{(math.fabs(result_tuple[2] - result_tuple[3]) / 10**6):.2f}**\" \n",
    "        }\n",
    "        if unified_config:\n",
    "            if unified_config.model.cls_head == \"attn\":\n",
    "                row['Classifier            '] = \"Attention head\"\n",
    "            if unified_config.model.cls_head == \"linear\":\n",
    "                row['Classifier            '] = \"Linear-layer head\"\n",
    "            row[\"Label #\"] = int(denoiser_config.model.parameters.label_num)\n",
    "            row[\"Convolutional kernel\"] = int(denoiser_config.model.parameters.patch_size)\n",
    "        else:\n",
    "            row['Classifier            '] = model\n",
    "            \n",
    "            # for paper substitute Zero-shot unified classifier with Zero-shot head\n",
    "            if model == \"Zero-shot unified classifier\":\n",
    "                row['Classifier            '] = \"Zero-shot head\"\n",
    "                \n",
    "            # for paper substitute Conventional separate classifier with Conventional model\n",
    "            if model == \"Conventional separate classifier\":\n",
    "                row['Classifier            '] = \"Conventional model\"\n",
    "                \n",
    "            row[\"Label #\"] = \"\"\n",
    "            row[\"Convolutional kernel\"] = \"\"\n",
    "            \n",
    "        # this mapping done for the paper, generalize in future\n",
    "        if result_tuple[0] == \"ot266\":\n",
    "            row['Allele'] = \"*vab-3(ot266)*\"\n",
    "        elif result_tuple[0] == \"G54\":\n",
    "            row['Allele'] = \"*him-4(u924)*\"\n",
    "        elif result_tuple[0] == \"G61\":\n",
    "            row['Allele'] = \"*mec-1(u925)*\"\n",
    "            \n",
    "        rows.append(row)\n",
    "        \n",
    "order = [\n",
    "    'Classifier            ',\n",
    "    # 'Label #', removed from paper\n",
    "    # 'Convolutional kernel', removed from paper\n",
    "    'Allele',\n",
    "    'Chrom',\n",
    "    'RD *mu*',\n",
    "    'Predicted *mu*',\n",
    "    '**Delta (Megabase)**',\n",
    "]\n",
    "        \n",
    "        \n",
    "dataframe = pd.DataFrame(rows)\n",
    "dataframe = dataframe[order]\n",
    "dataframe = dataframe.sort_values(by=['**Delta (Megabase)**']).reset_index(drop=True)\n",
    "\n",
    "os.makedirs(\"../metrics/result_tables\", exist_ok=True)\n",
    "markdown = dataframe.to_markdown(index=False)\n",
    "with open(\"../metrics/result_tables/genomes_results.md\", \"w\") as f:\n",
    "    f.write(markdown)\n",
    "\n",
    "pn.extension('tabulator', theme='dark')\n",
    "table = pn.pane.DataFrame(dataframe, sizing_mode=\"stretch_both\", max_height=1800)\n",
    "table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chromo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
